# 四、决策树

决策树表示给定特征条件下，类的条件概率分布。

## 1. 特征选择

特征选择是决定用哪个特征来划分特征空间。

### 1. 信息熵

当前 X 的不确定程度。
$$
H(X) = -\sum_{i=1}^{n}P(X=x_i)logP(X=x_i)
$$

### 2. 条件信息熵

引入条件 X 后，Y 的不确定程度。
$$
H(Y|X) = \sum_{i=1}^{n}P(X=x_i)H(Y|X=x_i)
$$

### 3. 信息增益

特征 A 的引入，对数据集 D 的不确定性的减少程度。也叫做互信息
$$
g(D,A) = H(D) - H(D|A)
$$
其中，
$$
H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
$$

$$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}
$$

### 4. 信息增益比

信息增益可能与当前特征的种类数量有一定的关系，所以引入信息增益比。
$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)}
$$

$$
H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$

## 2. 决策树生成

### 1. ID3 算法

使用信息增益作为衡量标准，划分特征。（找最大的信息增益，直到信息增益小于阈值）

### 2. C4.5算法

使用信息增益比作为衡量标准，划分特征。（找最大的信息增益比，直到信息增益比小于阈值）

## 3. 决策树剪枝

决策树学习的损失函数可以定义为：
$$
\begin{aligned}
C_\alpha (T) &= \sum_{t=1}^{|T|}N_tH_t(T)+\alpha |T|\\
&=  -\sum_{t=1}^{|T|}N_t\sum_{k=1}^K\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}+\alpha |T|\\
&=  -\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}log\frac{N_{tk}}{N_t}+\alpha |T|\\
&=  C(T) + \alpha |T| 
\end{aligned}
$$
其中，$ C(T)$ 表示模型对训练数据的预测误差，$\alpha |T|$ 表示模型复杂度。

通过调节 $\alpha$ 的大小可以调节剪枝后生成的决策树的规模。

在剪枝过程中，修建叶节点后计算损失，若比之前小，则确定修剪。

## 3. CART 算法

CART 算法对每一个特征进行最优切分，所以生成的决策树为二叉树。

而 ID3 和 C4.5 因为按照特征类别个数分类，所以生成的决策树为非二叉树。

### 1. 回归树

首先选择最优切分变量$j$和最优切分点$s$。

将输入空间划分为$R_1(j,s) = {x|x^{(j)}\le s},R_2(j,s) = {x|x^{(j)} > s}$两部分。

求解损失，使其最小：
$$
\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
其中，最小的 $c_1,c_2$ 为 $R_1,R_2$ 上输出值的平均值。

找到最优切分变量$j$和最优切分点$s$后，对于划分的区域确定输出值。（其实就是 $c_1,c_2$ ）
$$
\hat{c_m} = \frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i\space,\space m=1,2
$$
然后继续进行划分，最终得到生成决策树：
$$
f(x) = \sum_{m=1}^{M}\hat{c_m}I(x \in R_m)
$$

### 2. 分类树

#### 1. 基尼指数

$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$

所以对于训练集，有基尼指数：
$$
Gini(D) = 1-\sum_{k=1} ^K(\frac{|C_k|}{D})^2
$$
如果样本集合 D 被 A= a 分割成了两部分$D_1,D_2$，则在 特征 A 的条件下，集合 D 的基尼指数定义为：
$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

#### 2. 决策树生成

对于多种类特征，分别选中每一个类作为正样本，其他类均作为正样本，分别计算基尼指数，选择基尼指数最小的。

如 A = 1,2,3 时，需要计算$Gini(D,A=1)$，$Gini(D,A=2)$，$Gini(D,A=3)$三个基尼指数。



### 3. CART 剪枝

我们有决策树损失：
$$
C_\alpha(T) = C(T) + \alpha |T|
$$
我们对一个内部结点t，计算剪枝后（变为单节点子树）和剪枝前的损失，有：
$$
C_\alpha(t) = C(t)+\alpha \\\\
C_\alpha(T_t) = C(T_t) + \alpha|T|
$$
当$\alpha = 0$ 或 $\alpha$充分小时，有不等式：
$$
C_{\alpha}(T_t) < C_{\alpha}(t)
$$
当$\alpha$增大时，一定会使上述不等式相等，即$\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}$时。

这时剪枝前和剪枝后的决策树损失是相同的，而根据奥卡姆剃刀准则，我们要选择结构更加简单的，剪枝后的决策树。

所以我们记：
$$
g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}
$$
在剪枝过程中，我们首先定义$\alpha = +\infty$，然后计算 $g(t)$ ，并使：
$$
\alpha = min(\alpha ,g(t))
$$
将当前剪枝后的决策树和$\alpha$值做记录后，继续进行剪枝。

将所有经过剪枝的决策树，使用交叉验证的方法选出最优剪枝决策树，剪枝完成。
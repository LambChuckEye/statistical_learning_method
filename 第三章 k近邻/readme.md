# 二、k 近邻算法

k-NN算法可以进行分类，也可以进行回归。

k近邻算法不具有显式的学习过程。实际上是利用训练数据集实现分类过程，而不是学习出一个带有参数的模型。

## 1. 模型：

1. 根据给定的**距离度量**，在训练集 T 中找出与 x 最近邻的 **k 个点**，涵盖这 k 个点的 x 邻域记为 $N_k(x)$
2. 在 $N_k(x)$中根据**分类决策规则**，决定 x 的类别 y。

其中，距离度量、k值选择、分类决策规则，被称为k-NN算法的三个基本要素

## 2. 距离度量：

用于计算数据中两个样本点之间的距离，以决定如何判断最近邻。

常用的距离度量算法为$L_p$ 距离：
$$
L_p(x_i,x_j) = (\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}
$$
其中，当 p = 1 时，为曼哈顿距离：
$$
L_1(x_i,x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|
$$
当 p = 2 时，为欧氏距离
$$
L_2(x_i,x_j) = (\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}
$$
当 p = $\infty$ 时，为最大值距离
$$
L_\infty(x_i,x_j) = \max_l|x_i^{(l)}-x_j^{(l)}|
$$

## 3. k 值选择

- 当 k 值较小时，意味着模型变得更复杂，容易发生过拟合。
- 当 k 值较大时，意味着模型变得更简单，模型的预测能力下降。



## 4. 分类决策规则

常用多数表决规则作为 k-NN算法的决策规则。

在k-NN模型分类过程中，模型的误分类率为：
$$
\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\ne c_j)
$$
其等价于1 减去正确分类率：
$$
1- \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i =  c_j)
$$
所以，最小化误分类率，即最小化经验风险，就等价于使 $\sum_{x_i\in N_k(x)}I(y_i =  c_j)$ 最大，而该式子便是多数表决。

所以说，**多数表决规则等价于经验风险最小化**。

## 5. kd 树实现

### 1. kd 树构造

循环为每个维度进行排序，由其中位数作为当前节点。

小于中位数部分置于左子树队列，大于部分置于右子树队列。

知道所有样本均已入树。

### 2. kd 树搜索

1. 根据每层对应的维度进行搜索，直至叶子节点，将叶子结点标为已访问

   若最近邻队列不为空，或者当前叶子结点与输入样本的距离，小于最近邻队列中样本距离的最大值，则将该节点进入或代替最大距离样本进入队列中。

2. 判断当前叶子节点的父节点是否已访问：

   若已访问，则继续 2。

   若未访问，则将当前节点置为已访问，并：

   ​	a. 计算输入样本与当前维度中位切分线之间的距离：

   ​		若距离大于最近邻队列中的样本距离最大值，说明当前切分线另一侧不存在最近邻节点，继续 2。

   ​		若距离小于最近邻队列中的样本距离最大值，说明当前切分线另一侧存在最近邻节点，则对其另一子树方向执行 1。